{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tva04/share_code/blob/main/anand_gesture_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb4LZILIj6pO"
      },
      "source": [
        "# 0 - Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2815Y7rj9bV"
      },
      "source": [
        "- To recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
        "\n",
        "- The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
        "\n",
        "  - Thumbs up: Increase the volume\n",
        "  - Thumbs down: Decrease the volume\n",
        "  - Left swipe: 'Jump' backwards 10 seconds\n",
        "  - Right swipe: 'Jump' forward 10 seconds\n",
        "  - Stop: Pause the movie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwG0rrOYkow6"
      },
      "source": [
        "## 0.1 Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV8g4St-kt-S"
      },
      "source": [
        "The training data consists of a few hundred videos categorised into one of the five classes. Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images). These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use.\n",
        "\n",
        "Note that all images in a particular video subfolder have the same dimensions but different videos may have different dimensions. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos). Hence, you will need to do some pre-processing to standardise the videos.\n",
        "\n",
        "Each row of the CSV file represents one video and contains three main pieces of information - the name of the subfolder containing the 30 images of the video, the name of the gesture and the numeric label (between 0-4) of the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfnDD4Q0kasU"
      },
      "source": [
        "# 1 - Pre-requisite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_1XHtXZkcvh"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkjZV7hjj3-E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(30)\n",
        "rn.seed(30)\n",
        "tf.random.set_seed(30)\n",
        "\n",
        "# Import Keras from TensorFlow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K# 0:\n",
        "import datetime\n",
        "import imageio\n",
        "from skimage.transform import resize, rescale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ove8uKNglWDr"
      },
      "source": [
        "Load data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txt0oPcwjzFL"
      },
      "outputs": [],
      "source": [
        "train_doc = np.random.permutation(open('/home/datasets/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/home/datasets/Project_data/val.csv').readlines())\n",
        "batch_size = 10\n",
        "num_classes = 5\n",
        "curr_dt_time = datetime.datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GkNrei5P8JD",
        "outputId": "eb5116ed-d9c2-4373-f762-1bc3e89925c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n"
          ]
        }
      ],
      "source": [
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "train_path = '/home/datasets/Project_data/train'\n",
        "val_path = '/home/datasets/Project_data/val'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmYBRumlz9n"
      },
      "source": [
        "# 2. Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zlIh7pl69G"
      },
      "source": [
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with img_idx, y,z and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEtzKvjElyIY"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size, num_images=20, height=120, width=120, augment=False, total_frames=30, channels=3):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    #create a list of image numbers you want to use for a particular video\n",
        "    img_idx = np.round(np.linspace(0,total_frames-1,num_images)).astype(int)\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(t)//batch_size  # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_data = np.zeros((batch_size,num_images, height, width, channels))\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    image = resize(image, (height, width), anti_aliasing=True)\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    if augment:\n",
        "                        if np.random.randn() > 0:\n",
        "                            image = datagen.random_transform(image)\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = (image[...,0])/255 #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = (image[...,1])/255 #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = (image[...,2])/255 #normalise and feed in the image\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if (len(t)%batch_size):\n",
        "            remaining_batch_size = len(t)%batch_size\n",
        "            batch_data   = np.zeros((remaining_batch_size, num_images, height, width,3)) # 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((remaining_batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "\n",
        "            for folder in range(remaining_batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imageio.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    h, w, c = image.shape\n",
        "                    image = resize(image, (height, width), anti_aliasing=True)\n",
        "\n",
        "                    # Randomly transform few images of few folders; note that folders are randomly shuffled in each epoch too\n",
        "                    if augment:\n",
        "                        if np.random.randn() > 0:\n",
        "                            image = datagen.random_transform(image)\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = (image[...,0])/255 #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = (image[...,1])/255 #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = (image[...,2])/255 #normalise and feed in the image\n",
        "                    #batch_data[folder,idx,:,:,0] = (image[...,0] - image[...,0].min())/(image[...,0].max() - image[...,0].min())#normalise and feed in the image\n",
        "                    #batch_data[folder,idx,:,:,1] = (image[...,1] - image[...,1].min())/(image[...,1].max() - image[...,1].min())#normalise and feed in the image\n",
        "                    #batch_data[folder,idx,:,:,2] = (image[...,2] - image[...,2].min())/(image[...,2].max() - image[...,2].min())#normalise and feed in the image\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ2rFb3orVXg"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj9ft5YUry_a"
      },
      "source": [
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cElTV1YUl5-1"
      },
      "outputs": [],
      "source": [
        "# Import Keras from TensorFlow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06_Ygut8sCg9"
      },
      "outputs": [],
      "source": [
        "def conv3D(conv_filters=(16, 32, 64, 128), dense_nodes=(256,128), dropout=0.25, num_images=20, height=120, width=120):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv3D(conv_filters[0], (3, 3, 3), padding='same', input_shape=(num_images, height, width, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "    model.add(Conv3D(conv_filters[1], (3, 3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "    model.add(Conv3D(conv_filters[2], (3, 3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "    model.add(Conv3D(conv_filters[3], (3, 3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_nodes[0]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(dense_nodes[1]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    opt = optimizers.Adam() #write your optimizer\n",
        "    model.compile(opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmU1JLFNsdoI"
      },
      "source": [
        "Conv 3D Model with 30 frames per video (16, 32, 64, 128 filters conv 3D layers + 256 dense layer + 128 dense layer + image size 120 by 120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXn_j9o-sNyG",
        "outputId": "9d762def-8a1f-4701-8504-139817f68862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# epochs = 20\n",
            "# batch size = 10\n",
            "# num_frames per video = 30\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 30, 120, 120, 16)  1312      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 30, 120, 120, 16)  0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 120, 120, 16)  64       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 15, 60, 60, 16)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 15, 60, 60, 32)    13856     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 15, 60, 60, 32)    0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 15, 60, 60, 32)   128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 7, 30, 30, 32)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 7, 30, 30, 64)     55360     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 7, 30, 30, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7, 30, 30, 64)    256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 3, 15, 15, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 3, 15, 15, 128)    221312    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 3, 15, 15, 128)    0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 3, 15, 15, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling3d_3 (MaxPooling  (None, 1, 7, 7, 128)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1605888   \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,933,765\n",
            "Trainable params: 1,932,517\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-30 02:14:16.999546: W tensorflow/stream_executor/platform/default/dso_loader.cc:65] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-03-30 02:14:16.999585: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2025-03-30 02:14:16.999611: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20 # 25 choose the number of epochs\n",
        "print ('# epochs =', num_epochs)\n",
        "batch_size = 10  #experiment with the batch size\n",
        "print ('# batch size =', batch_size)\n",
        "num_frames = 30\n",
        "print ('# num_frames per video =', num_frames)\n",
        "height = 120\n",
        "width = 120\n",
        "\n",
        "model = conv3D(num_images=num_frames)\n",
        "\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvxpjScpP8JH"
      },
      "source": [
        "## 3.1 Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KrUA1dmP8JH"
      },
      "outputs": [],
      "source": [
        "def model_callbacks(folder_name):\n",
        "    model_name = str(folder_name) + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "    if not os.path.exists(model_name):\n",
        "        os.mkdir(model_name)\n",
        "\n",
        "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
        "\n",
        "    return [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7jgblZIP8JH"
      },
      "outputs": [],
      "source": [
        "def calculate_steps(num_train_sequences, num_val_sequences, batch_size):\n",
        "    if (num_train_sequences%batch_size) == 0:\n",
        "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "    else:\n",
        "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "    if (num_val_sequences%batch_size) == 0:\n",
        "        validation_steps = int(num_val_sequences/batch_size)\n",
        "    else:\n",
        "        validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "    return steps_per_epoch,validation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVSWYY1gP8JH",
        "outputId": "e3fa00d9-61c1-4bfd-bea2-c63495504a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "callbacks_list = model_callbacks(\"model_conv3D_1\")\n",
        "\n",
        "steps_per_epoch, validation_steps = calculate_steps(num_train_sequences, num_val_sequences, batch_size)\n",
        "\n",
        "train_generator = generator(train_path, train_doc, batch_size, num_images=num_frames)\n",
        "\n",
        "val_generator   = generator(val_path, val_doc, batch_size, num_images=num_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlpGIV7IP8JI",
        "outputId": "0d599815-a846-4b8b-cb54-9ab93ae54cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  /home/datasets/Project_data/train ; batch size = 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_279/2636822326.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "10/67 [===>..........................] - ETA: 19:23 - loss: 1.9477 - categorical_accuracy: 0.3200"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                             callbacks=callbacks_list, validation_data=val_generator,\n",
        "                             validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_accuracy(history):\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JEfghOxrQTWl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDJ8Nie3P8JI"
      },
      "outputs": [],
      "source": [
        "plot_loss_accuracy(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Conv 3D Model with 20 frames per video (16, 32, 64, 128 filters conv 3D layers + 256 dense layer + 128 dense layer + image size 120 by 120)"
      ],
      "metadata": {
        "id": "s5rmPPFQQhVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQjxJxTbP8JI"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20 # choose the number of epochs\n",
        "#print ('# epochs =', num_epochs)\n",
        "batch_size = 10  #experiment with the batch size\n",
        "print ('# batch size =', batch_size)\n",
        "num_frames = 20\n",
        "print ('# num_frames per video =', num_frames)\n",
        "height = 120\n",
        "width = 120\n",
        "\n",
        "model = conv3D(num_images=num_frames)\n",
        "\n",
        "print (model.summary())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}